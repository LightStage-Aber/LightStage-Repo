

LightStage_LightPositioning_EvaluationTool
------------------------------------------


== FILES:
-----------
    /src                                    - Code.
    /test                                   - Unit tests for src/ code.
    /models                                 - Sample 3D obj model files.
    /results                                - Example scoring result data files and example input .csv/.obj files.
    /properties								- Configuration file to simplify running of experiments; in particular with command line options: -e3 through to -e9.
    /exp                                    - Experiment scipts, i.e. for particle replusion position testing



== DEMO USAGE EXAMPLES:
-------------------
    python run.py -h
        Displays options on command line.

    python run.py
        Run in display mode (-m), with default mini-dome, default camera layout (-c) and default required LEDs (-l).

    python run.py -s 2 -t '(1,0,0)' -c 2
        Run in display mode, with a mini-dome scaled (-s) by 2, moved to position '1' on X-axis, with camera layout (-c) as 'even bias'.

    python run.py -p "../models/dome/dome_c.obj" -s 2 -t '(0,0,0)' -l 41 -r ../results/blinn-phong+lambert_shading_scores/dome_realistic_cam/led_scores_2016-01-08-21-16-06.csv
        Run in display mode, with the dome from its obj file (-p), scaled (-s) by 2, not moved (-t), with 41 leds (-l) selected.
        Loads result data file (-r) for the dome model: ../blinn-phong+lambert_shading_scores/dome_realistic_cam/led_scores_2016-01-08-21-16-06.csv

    python run.py -p "../models/Flower/plants3.obj" -s 0.01 -t '(0,-4,0)' -r ../results/blinn-phong+lambert_shading_scores/plant3_realistic_cam/led_scores_2016-01-11-01-22-23.csv
        Run in display mode, with plants3.obj target model loaded (-p), with a scaling factor (-s) of 0.01 and translated (-t) -4 on the Y-axis
        Loads result data file (-r) for the plant3 model: ../results/blinn-phong+lambert_shading_scores/plant3_realistic_cam/led_scores_2016-01-11-01-22-23.csv



== EVALUATION USAGE EXAMPLES USING ILLUMINANCE:
-----------------------------------------------

    This is for evaluating LED positions by the amount of light absorbed by each surface of the target object model.
    This type of evaluation is not affected by surface materials.
    To visually see what will be evaluated, use both the -m1 and -eX command line options. Then to evaluate that (recreated) scene, use -m2 -eX.
    
    == Evaluate Index Positions: (-e3 / -e4)

    python run.py -m 2 -e 3
        Run in evaluation mode (-m), using positions loaded from an CSV index file (-e) and expecting those indexes to correlate to the 92 joints of the geodesic dome (-e3).
        Alternatively, run with (-e4) where the expected indexes will correlate to 3991 positions on the bars connecting the joints of the geodesic dome.
        Requires configuration value in 'properties/default.properties' file:
        - Under the [LightIndexPositions] section, as:
            - results_file.column_number=3
                - This is the column index of the CSV file. A cell value of '1' or 'True' is considered selected.
            - results_file.number_of_leds=44
                - This is the number of LEDs expected, and limits the selected LEDs up to this number.
            - results_file.results_output_file_path_prefix=results/Control_91-92_March2017/
                - This is the output directory for the evaluation results, written to a file in CSV format.
                - For -e3 the output filename is 'lambertian_led_sets_VertexIndexPositionEvaluator.csv'
                - For -e4 the output filename is 'lambertian_led_sets_Edge10IndexPositionEvaluator.csv'
            - results_file.csvfilename=../results/Control_91-92_March2017/l92.csv
                - This is the input CSV file from which the indexes will be read.
        - Under the [FrameModel] section, as:
            - frame.objfilename=../models/dome/dome_c.obj
                - This is the frame model, to which the index position are mapped.
            - frame.scale=8
                - This is the scale of the frame model, relative to the target model.
            - frame.withsupportaccess=True
                - This determines if the lowest joint of the geodesic dome is removed, i.e. an opening to access the inside of the dome.
            - frame.indexes_are_important=False
                - This applies a number mapping to the joints, as used in AU's actual dome (06/2016). Must be False used when mappings to bars are used.
        This takes from 1 to 180+ seconds for evaluation depending on number of LEDs and number of surfaces of target model.
        By example, 44 LEDs and the default dome model with 180 surfaces, takes 1-2 seconds.


    == Evaluate 3d Coordinate Positions from .obj file format: (-e7, -e8, -e9)

    python run.py -m 2 -e 7
        Run in evaluation mode (-m), using positions loaded from an .obj file.
        Use (-e7) to evaluate the 'Raw' positions from loaded 3d coordinates to the target model.
        Use (-e8) to evaluate the 'VertexMapped' positions, where each of the loaded 3d coordinates are mapped to the closest of 92 dome frame joint (vertex) based on distance using Euclidean distance.
        Use (-e9) to evaluate the 'VertexMapped' positions, where each of the loaded 3d coordinates are mapped to the closest of 3991 dome frame positions on the bar between joint (Edge10) based on distance using Euclidean distance.
        Requires configuration value in 'properties/default.properties' file:
        - Under the [LightPositions] section, as:
            - light.results_output_file_path_prefix=results/Sample_Lettvin_Results_Mar2017/
                - This is the output directory for the evaluation results, written to a file in CSV format.
                - For -e7 the output filename is 'lambertian_led_sets_RawPositionEvaluator.csv'
                - For -e8 the output filename is 'lambertian_led_sets_VertexMappedPositionEvaluator.csv'
                - For -e9 the output filename is 'lambertian_led_sets_Edge10MappedPositionEvaluator.csv'
            - light.scale=8
                - This is the scale of the light model vertex positions, and will be relative to the target model.
            - light.objfilename=../results/Sample_Lettvin_Results_Mar2017/test_tuning_n7_j0-001.txt.obj
                - This is the input .OBJ file from which the vertex positions ('v x y z') will be read.
        - Under the [FrameModel] section, as:
            - frame.objfilename=../models/dome/dome_c.obj
                - This is the frame model, to which the index position are mapped.
            - frame.scale=8
                - This is the scale of the frame model, relative to the target model.
            - frame.withsupportaccess=True
                - This determines if the lowest joint of the geodesic dome is removed, i.e. an opening to access the inside of the dome.
            - frame.indexes_are_important=False
                - This applies a number mapping to the joints, as used in AU's actual dome (06/2016). Must be False used when mappings to bars are used.
        This takes from 1 to 180+ seconds for evaluation depending on number of LEDs and number of surfaces of target model.
        By example, 44 LEDs and the default dome model with 180 surfaces, takes 1-2 seconds.



== EVALUATION USAGE EXAMPLES USING LUMINANCE:
---------------------------------------------

    This is for viewing and evaluating LED positions by the amount of light reflected to the camera positions.
    To visually see what will be evaluated, use both the -m1 and -eX command line options. Then to evaluate that (recreated) scene, use -m2 -eX.

    python run.py -m 2 -e 1
        Run in evaluation mode (-m) to measure reflectance (-e1), with default mini-dome, default camera layout (-c) and default required LEDs (-l).
        This took 75 seconds, during testing.
        Created information file: ../led_meta_data_YYYY-MM-DD-hh-mm-ss.txt
        Created result data file: ../led_scores_YYYY-MM-DD-hh-mm-ss.csv

    python run.py -m 2 -e1 -p "../models/Flower/plants3.obj" -s 0.01 -t '(0,-4,0)'
        Run in evaluation mode (-m) to measure reflectance (-e1), with plants3.obj target model loaded (-p), with a scaling factor (-s) of 0.01 and translated (-t) -4 on the Y-axis.
        This took 83 minutes to run. Default is 'realistic bias' camera layout (-c), with 42 LEDs required (-l).
        Created information file: ../led_meta_data_YYYY-MM-DD-hh-mm-ss.txt
        Created result data file: ../led_scores_YYYY-MM-DD-hh-mm-ss.csv


    These are for viewing LED positions, loaded from a luminance result file:

    python run.py -p "../models/dome/dome_c.obj" -s 2 -t '(0,0,0)' -l 41 -r ../results/blinn-phong+lambert_shading_scores/dome_realistic_cam/led_scores_2016-01-08-21-16-06.csv
        Run in display mode, with the dome from its obj file (-p), scaled (-s) by 2, not moved (-t), with 41 leds (-l) selected.
        Loads result data file (-r) for the dome model: ../blinn-phong+lambert_shading_scores/dome_realistic_cam/led_scores_2016-01-08-21-16-06.csv

    python run.py -p "../models/Flower/plants3.obj" -s 0.01 -t '(0,-4,0)' -r ../results/blinn-phong+lambert_shading_scores/plant3_realistic_cam/led_scores_2016-01-11-01-22-23.csv
        Run in display mode, with plants3.obj target model loaded (-p), with a scaling factor (-s) of 0.01 and translated (-t) -4 on the Y-axis
        Loads result data file (-r) for the plant3 model: ../results/blinn-phong+lambert_shading_scores/plant3_realistic_cam/led_scores_2016-01-11-01-22-23.csv

    When loading a luminance result file (as above), press F1 to view the hotkeys:
        - 'l' to load a new result data file.
        - 'r' to change between the data file's LED scoring measures.
            - (1) ranked maximised intensity. LEDs are selected during Display Mode runtime, based on rank.
                  Use +/- to change the quantity of ranked LEDs displayed.
            - (2) minimised coverage error, default of 42 LEDs is set.
                  LEDs displayed under this scoring have been selected during the Evaluation Mode run.




== BRIGHTNESS CONTROL TUNING USAGE EXAMPLE USING ILLUMINANCE:
-------------------------------------------------------------

    This provides a search to improve the evaluation score of a given set of LED positions, by adjusting the brightness of each LED.
    The (-m2) evaluation step will produce the default brightness intensities per LED position as (1,1,1,1,1,1, etc.), these will be used as the initial search starting point.
    Starting a brightness control tuning search will tune or update the intensity values, such that the normalised standard deviation illuminance measure improves (reduces).
    The output of the search will be adjusted brightness values per LED (e.g. 1.1,1.0,1.0,0.9,0.8,0.8,0.9,1.0,1.0,1.1, etc.).
    To visually see the positions that will be evaluated, use both the -m1 and -eX command line options. Then to tune that (recreated) scene, use -m3 -eX.

    python run.py -m 3 -e 7
        Run in brightness control tuning mode (-e7) using positions loaded from an .obj file. Any of the evaluations modes above (3,4,7,8,9) can be used here.
        The tuning must evaluate using the same method as -m2. Therefore it's configuration must be correct. Equally, -m1 to demo the configuration will help show if the configuration is visually accurate.
        Requires configuration in 'properties/default.properties' file:
        - Under the [BrightnessControlTuner] section, as:
            - tune.debug=True
                - This will print out debug information to STDOUT showing progress through-out the tuning search, as control level. Each search strategy has it's own debug statements.
            - tune.mode="L-BFGS-B"
                - The mode determines which search strategy is used. "L-BFGS-B" is a global scipy search. "IterativeRegression" is a local (faster) numeric optimisation search.
        - "IterativeRegression" strategy has the following parameters:
            - tune.regression.threshold=0.0005
                - The stopping evaluation score threshold.
            - tune.regression.max_iterations=1000
                - The maximum iterations of regression to complete, if stopping threshold is not exceeded.
            - tune.regression.debug=False
                - Print out progress and evaluation scores as search progresses.
        - "L-BFGS-B" strategy has the following parameters. See the scipy.optimize.basinhopping documentation for details.
            - tune.scipy.basinhopping.niter=0
            - tune.scipy.basinhopping.niter_success=1
            - tune.scipy.basinhopping.t=0.5
                - See parameter 'T'.
            - tune.scipy.basinhopping.disp=False
            - tune.scipy.basinhopping.lower_bounds=0.5
                - The lower bound for brightness lumens, applied for all LEDs. 1.0 is the relative and default lumen level.
            - tune.scipy.basinhopping.upper_bounds=1.5
                - The upper bound for brightness lumens, applied for all LEDs.



== FURTHER INFORMATION ON LUMINANCE EVALUATION MODES (Temporarily disabled feature in v0.1.8):
----------------------------------------------------------------------------------------------

    == EVALUATION MODE:
        - Option: (-m2 -e2)
		    - This mode will evaluate and log results to file evaluating ray reflection intensities into the observing cameras.
		    - For a simple target model with 180 triangle it took 1.5 minutes to evaluate.
		    - The result data file and meta data files are stored in '../led_scores_<timestamp>.csv' and '../led_meta_data_<timestamp>.txt'
		    - The evalution depends on:
		        - camera positions
		        - 6 rotations of the target model within the LightStage.
		        - LED and quantity of LEDs desired
		        - target model, positioning (scale and translation inside dome)
		    - *We recommended that the camera layout and model positioning be set and previewed in the Display Mode before running in Evaluation Mode.*
		    - The evaluation measures:
		        - the light intensity score measure, uses Lambertian and Blinn-Phong reflection models to accumulate diffuse and specular
		          reflection intensities per tri, per LED.
		        - the coverage error scoring measure, minimises the quantity of unlit target model triangle surfaces. This is done by
		          prioritising LEDs that are alone in lighting a tri surface, followed by prioritising LEDs that have a high
		          AccumIntensity * QtyTriHit score. With a limited search step and search depth, the best scoring set of LEDs are recorded,
		          of quantity (-l).
		          + AccumIntensity is the reflection intensity score per LED per tri of the target model, accumulated over the quantity of target model rotations.
		          + QtyTriHit is the quantity of tris of the target model that are lit by the LED, accumulated over the quantity of target model rotations.




=== COMMAND LINE HELP OUTPUT:
-----------------------------

$ python run.py -h
Usage: run.py [options]

Options:
  -h, --help            show this help message and exit
  -m NUM, --mode=NUM    Specify the tool mode.
                        1=Display mode (default).
                        2=Evaluation mode.
                        3=Tune-evaluation mode.
  -e NUM, --evaluation-metric-mode=NUM
                        Specify the evaluation metric mode.
                        '-e0' =Default. (-m1) Display best selected LEDs from -e1 result file. (-m2) No action.
                        '-e1' =Use Reflectance Measures. 
                        '-e2' = (Disabled) Use Illuminance Measure Monte Carlo Search (Lambertian only).
                        '-e3' =Use Illuminance Measure (Lambertian only) to evaluate a loaded file containing indexed positions;
                        depends on properties file values: see '[EvaluateSingleResultsFile]'
                        and '[FrameModel]' - (file, qty, column, etc.) in 'default.properties' file.
                        '-e4' =Use Illuminance Measure (Lambertian only) to evaluate a loaded file containing indexed positions;
                        depends on properties file values: see '[EvaluateSingleResultsFile]'
                        '-e7' =Use Illuminance Measure (Lambertian only) to evaluate 'RAW' vertex 
                        position file; depends on properties file values: see '[LightPositions]'
                        '-e8' =Use Illuminance Measure (Lambertian only) to evaluate 'Vertex
                        Mappings' vertex position file; depends on properties file values: see '[LightPositions]'
                        '-e9' =Use Illuminance Measure (Lambertian only) to evaluate
                        'Edge(10) Mappings' vertex position file; depends on properties file values: see '[LightPositions]'
  -p PATH, --target-path=PATH
                        Specify the path to an .obj file of the target model.
                        For example: '../models/dome/dome_c.obj'. Default is a mini-dome.
  -s SCALE, --target-scale=SCALE
                        Specify the target model scaling factor. - Default=1.0
  -t (x,y,z), --target-translation=(x,y,z)
                        Specify the target model translation. i.e. move -1 on
                        Y-axis: '(0,-1,0)'
  -c NUM, --camera-layout=NUM
                        Specify the camera layout. 1=Realistic bias (default).
                        2=Even bias.
  -l NUM, --qty-leds=NUM
                        Specify the number of LEDs to show in demo and to
                        evaluate in luminance mode, in range(0-92). Default is 44.
                        *Note: This value is not currently recalled from any results
                        files.*
  -r PATH, --load-result-file=PATH
                        Specify the path to an LED score result data file. For
                        example: '../led_scores_xxx.csv'.
  -k NUM, --reflectance-score=NUM
                        Specify the reflectance model scorings used in
                        evaluation mode. 1=Lambert's diffuse only. 2=Lambert's
                        diffuse and Blinn-Phong's specular (default).
  -d NUM, --display-evaluation-metric=NUM
                        Specify the evaluation metric display mode from result
                        file. 1=Use Index Column 1 Scores (default). 2=Use
                        Index Column 3 Scores (Lambertian only).






============================
=== CHANGE LOG HISTORY:
============================

Todo: Planned release: LightStage_LightPositioning_EvaluationTool v0.1.82 -- 2017/05/
Todo: - Integrate Saff97 spheres, via antiprism.
Todo: - Integrate Repulsion by percentage move, via antiprism.
Todo: - Create test suites for PLOS One paper including new Saff97 (as Geometric) and Repulsion by% with byJitter (as Repulsion).
Todo: - Create test suites for PLOS One paper with new target models (plant dataset, head/face dataset, sphere).

Todo: Planned release: - LightStage_LightPositioning_EvaluationTool v0.1.81 -- 2017/05/
Todo: - Refactor - Move dependencies of "Old_ToolSelector" (i.e. luminance evaluation and viewing techniques) from input arguments (and as globals) to properties key-value pairs. Remove capability to set number of LEDs in both locations.


LightStage_LightPositioning_EvaluationTool v0.1.8 -- 2017/06/26 - (2017/04/25)
------------------------------------------------------------------------------
Major additions:
- Created test suites for PLOS One paper Control,MC,Lettvin experiments. Reevaluated Control and Monte Carlo experiment results.
- Added /exp/ directory for integration of external tools.
- Added Lettvin's Repulsion algorithm (cpp) external tool. Edge10Mappings can be reevaluated with the `run_range_test_lettvin_diffuse.sh` bash script.
- Added 'Brightness Control Tuning' feature. Has two search tools: a local (numerical) iterative regression ("IterativeRegression") and a scipy.basinhopping global minimization search ("L-BFGS-B"). The search "mode" type and associated parameters can be specified in properties/default.properties config file.
- Created test suites for tuning tests - showing that balance evaluation score is reduced by the search.
- Created test suites for tuning installed (MonteCarlo) positions - showing that Monte Carlo positions can be improved by tuning.

Major changes:
- Refactorings from procedural to object oriented paradigm. This was done to support code state correctness and flexibility under repeat testing while properties configurations are affected per unit test of the same suite.
- Sped up visualisation with cached loaded (from ~1 FPS to ~30FPS). Added ToolSelector cache class to improve performance / remove requirement of file I/O every frame, (from ~1fps to ~30fps). Further improvements with "modern" opengl implementation (i.e. single glBegin/End batch call).
- Bug fixed: bug in Edge-point determination/ definition; when support access is enabled, all edges connected to the removed support access vertex are ignored (not created).
- Bug fixed: ensured correct usage of "loaded OBJ dome model vs hard coded dome model" for target and frame.
- Simplified the default.properties structure into:
    - [LightPositions] for vertex-based .obj files, (-e7,8,9)
    - [LightIndexPositions] for index based .csv files, (-e3,4)
    - [FrameModel] for the geodesic frame model
    - [BrightnessControlTuner] for tuning method type and parameters (-m3)
- Added 'frame.withsupportaccess' (bool) to default.properties file, loaded and used -e3,4,7,8,9. Which will remove the bottom (lowest Y-axis vertex) of the frame.
- Documenting here - 'frame.indexes_are_important' (bool) for default.properties file.
    - If true, use the mapped numbering system defined in 'src/data_3d/dome_obj_data.py' to associate vertex position indexes to the 0 to 91 vertex positions.
    - Must be 'False' for any Edge mappings (to points on the bar) because the mapped numberings only reach 92 positions, not 3991.
    - Optional for Raw 3d point to mapped Vertex Mappings.

Minor changes:
- Added cached objects and added unit tests that depend on STDOUT.
- Open issue: The hardcoded dome model (in 'src/data_3d/dome_obj_data.py') has one missing tri on the model.
    - Workaround: Only use loaded .obj models as target model. (loaded via -p)
    - Do not use the hardcoded dome as a target model.
        - This has been enforced in tool_manager.py->get_target_shape_triangles().
        - Details:
            - The missing tri face is lost when the .obj faces are remapped.
            - The vertex mappings (by AOS 06/2016) to the physical dome are accurate.
            - The missing tri affects the Face-to-Edge algorithm (i.e. Edge10) and evaluations using it as a target model.
- Restructured /modes/ directory modules into:
    - /luminance/ for original evaluation tools based on reflections into camera positions (old)
    - /illuminance/ for current evaluation tools based on lumens into target surfaces.
    - /brightness_control_tunings/ for search code strategies.



    
LightStage_LightPositioning_EvaluationTool v0.1.7 -- 2017/04/01
---------------------------------------------------------------
- Increased clipping distance ten fold to aid model loading and debugging.


LightStage_LightPositioning_EvaluationTool v0.1.6 -- 2017/03/29
-----------------------------------------------------------------------------------------------------------------------------------------------
- Added mapping of vertex positions loaded from .obj file to dome edges. Use with '-e9' command line option and configured using default.properties file.
- Minor changes for experimental testing and to documentation. Results not included in commit.


LightStage_LightPositioning_EvaluationTool v0.1.5 -- 2017/02/03
-----------------------------------------------------------------------------------------------------------------------------------------------
- Added loading explicit light (FrameModel) vertex positions from an .obj file, via the .ini format file at "../properties/default.properties".
- Evaluate those light positions with standard deviation of lambert diffuse reflectance (illuminance only) in class 'MeasureLoadedLightPositions' in illuminance.py.
- Map loaded light vertex positions (from file) to the closest frame joint positions (from "/data_3d/dome_obj_data.py").

    
-----------------------------------------------------------------------------------------------------------------------------------------------
LightStage_LightPositioning_EvaluationTool v0.1.4 -- 2017/02/03
- Renamed.
- Refactored code after new illuminance metrics and search method implementations were trialed. These new method implementations are not made available in this release.
- Note that the mappings in 'dome_obj_data.py' have been modified (by AOS) to match the actual dome vertex numberings.
- Coming up next: evaluate a loaded set of vertices. Map loaded set of vertices to possible frame mounting positions.
    
    
    
-----------------------------------------------------------------------------------------------------------------------------------------------
LightStage_IlluminationOfTargetObject v0.1.3 -- 2016/04/15
- Add Lambertian illumination (incidence) scores.
	- With a lambertian-like illuminance approach.
    - Use a fragment shader scoring approach to measure the illumination. Use lambertian score (~cosine of incidence) score at each surface.
    - Defaults to lambertian evaluations. See command line option (-k).
    
    

-----------------------------------------------------------------------------------------------------------------------------------------------
LightStage_IlluminationOfTargetObject v0.1.2 -- 2016/03/08
- Added Lambertian only reflectance scoring to evaluation mode. 
	- Added -k command line option to use this. 
- Included new result file for dome object model evaluation using Lambertian reflectance only.



-----------------------------------------------------------------------------------------------------------------------------------------------
LightProjection_on_Object v0.0.2 -- 2015/10/17
- Translated 'dome_c.obj' blender dome object into python functions to get dome vertices and edges/ faces. These vertices will be the LED light source vertices.
        @see dome_obj_data.py
- Calculated and drew lines to represent angles of incidence (from LED vertices) to a sample triangle face. This required functions to calculate the normalised perpendicular vector, the surface normal.
- Added a console printed boolean quantity measure of illumination. 1, if the projected LED angle of incidence is against the front face of the surface, else 0.
- Added 1 camera for future measure considering reflection angle into the camera.



LightProjection_on_Object v0.0.1 -- 2015/10/08
- Adapted a sample Py-OpenGL tutorial script to handle display loop, scene lighting and viewport manipulation via keyboard inputs, etc. 
        @see show_sample_reflection_opengl.py
- The handler module calls another module to manage drawing of specific objects in the scene and the model calculations. 
        @see draw_calc_lighting.py


To start with we'll use python 2.7 and opengl.
        sudo apt-get install python2.7 python-opengl numpy

